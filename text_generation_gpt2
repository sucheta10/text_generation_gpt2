from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load the tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # loads the pre-trained tokenizer for distilgpt2
model = GPT2LMHeadModel.from_pretrained('gpt2') # loads the pre-trained GPT-2 language model.
# tokenizer- breaks the inputed text into small tokens 
# model-learns patterns in text data during training and based on the learned patterns, models can generate predictions or outputs from given inputs



# Input text
#text = "Autobiography "
text = "Once upon a time"

# Tokenize the input text
encoded_input = tokenizer.encode(text, return_tensors='pt')# converts the input text into token IDs that the model can process.
# return_tensors='pt'---->specifies that the output should be a PyTorch tensor, which is the format the model expects.

# Generate text
output = model.generate(encoded_input, max_length=200,do_sample=True) # This generates text based on the input tokens.
# encoded_input--->tokenized input text
# max_length=200--->The maximum length of the generated sequence is 200
# do_sample=True--->Enables sampling instead of using greedy decoding. 
                    #This allows the model to generate more diverse text, as it introduces randomness in the selection of the next word.

# Decode the generated text
generated_text = tokenizer.decode(output[0], skip_special_tokens=True) # This converts the generated token IDs back into a human-readable string.
# output[0]--->The generated token IDs from the model
# skip_special_tokens=True--->removes any special tokens that were added by the tokenizer during processing, making the output text cleaner

# Print the generated text
print(generated_text)
